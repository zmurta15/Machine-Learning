Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the datasets provided, explain the need to standardize the attribute values.
R1: The features of the datasets provided have significant range differences in their values, and by doing standardization, the features's values are balanced to a common scale, so that they have a mean as 0 and a variance as 1.


Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: On the classification problem, we decided to standardize the features by subtracting each data row with the mean of the features, and next divide this value by the standard deviation. The class labels provided were not necessary to standardize since these are binary classification labels, meaning that they can only have the value of 0 or 1.
We did the described process for both the training and test sets.
On the regression problem, we opt to go with the StandardScaler fit_transform method, available on the sklearn.preprocessing module. This time we standardize the features and also the miss distance (the value we want to predict). This was done for the training and test sets.
The StandardScaler, suggested by the tutor, automatizes the process of calculating the mean and standard deviation of the sets.


Q3: Classification: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: For each class, the prior probability of an example belonging to that class is calculated by dividing the number of elements of the training set that belongs to that class (features_tr_1), by the total number of elements in the training set (features_tr).
Next follows our implementation to calculate the prior probability of class 1: prob_1 = len(features_tr_1)/len(features_tr).
The same goes to the probability of class 0 (features_tr_0).


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: To predict the class to which a test example belongs we add the the prior probability (explained in the question 3) to the probability of each attribute belonging to that class.
Then the probabilities of belonging to one of the two classes (0 or 1) are compared. The one with the higher probability is the one that the example is classified to.
Next follows the code depicting the explanation above:
for i in range(len(features_training[0, :])):
    kde1.fit(features_tr_1[:, [i]])
    kde0.fit(features_tr_0[:, [i]])
    prob_1 += kde1.score_samples(features_test[:, [i]])
    prob_0 += kde0.score_samples(features_test[:, [i]])
predicted_labels = []
for i in range(len(prob_1)):
    if (prob_1[i] >= prob_0[i]):
        predicted_labels.append(1)
    else:
        predicted_labels.append(0)
Prob_1 (and prob_0) are initialized with the log of the prior probability of belonging to each class. The score samples are the likelihood of each atribute.


Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: The bandwidth parameter determines the width of our kernel function. Using a small bandwidth value, we can get over-fitting on the training data since the density plot would get undersmoothed, looking like individual peaks around the data.
Using a higher bandwidth value, we normally get underfitting because the density curve tends to be oversmoothed, and end up not fitting enough to the training data.
Due to this, the classifier tends to be very good on the training data with a low bandwith but it will perform not so good on the validation and test set.   


Q6: Explain how you determined the best bandwidth parameter for your classifier. You may include a relevant piece of your code if this helps you explain.
R6: To determine the best bandwidth parameter for our classifier we used cross-validation to be able to get the lowest validation error from all the bandwidth possible values.
The value of the bandwidth that gets us the minimum cross-validation error will be choosen as our parameter value.
Next follows the code depicting the explanation above:     
if va_err < best_error:
    best_error = va_err
    choosen_bandwith = aux_h


Q7: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R7: For our implementation of the Naive Bayes Classifier using kernel density estimation, the only parameter that we needed to optimize was the bandwidth.
We tested, as requested, the bandwidth between 0.02 and 0.6, and used cross-validation to estimate the validation error and given the lowest we choose the best bandwidth.
The best hypothesis is then the kernel density function with the 'choosen_bandwith' that is used to calculate the likelihood of each feature.
In the GaussianNB from the sklearn library, the best hypothesis is calculated as the training data is fitted into the model and choosed to estimate the best predictions. 


Q8: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the one provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R8: Best bandwidth value: 0.26
Estimate the true error: 0.09943865276663999
Error of the gaussianNB:  0.14915797914995987
Normal Test NB: 124 - 20.712072867531337 // 124 + 20.712072867531337
Normal Test GNB: 186 - 24.656816817207723 // 186 + 24.656816817207723
Mcnemar's test (NB VS GNB):  47.705128205128204
The best bandwidth value is choosen with the validation set used in cross validation.
The true error (of our implementation) is calculated with the test set and has an accuracy of, more or less, 90%.
The test error of the Sklearn library GaussianNB has a higher error than our implementation.
This can be concluded by analysing the normal test where the intervals of the normal test never intersect and it can be concluded than our implememtantion is slight better at predicting the right class.
The same conclusion is done with Mcnemar's test because 47 is higher than 3.84 we can reject the null hypothesis and conclude that the classifiers do not perform identically and ours is better.



Q9: Regression: Explain what experiments and plots gave you good
evidence to choose a given model degree. 
R9: For each model degree, we computed the cross-validation in order to get the validation error for each degree. We choose the best degree by checking the minimum validation error. 
This is also possible to observe in the plot 'REGRESS-TR-VAL.png' where the red curve represents the validation error (mean squared error), and the best degree, x-axis, is where the curve hits the lowest value in the y-axis.
This is the best way to choose the model degree because using a validation set gives a validation error  unbiased by the training set which fitted the regression.  


Q10: In the case of your mean squared error plot explain why one of the error
curves is always decreasing, while the other is not.
R10: The validation error curve eventually begins to increase as the model degree increases but the training curve is always decreasing.
This is due to overfitting, because the model adapts to the details of the training set in that degree that do not generalize to the universe from which the data was sampled.


Q11: In the plots of the true versus predicted values, where would be
all the points when predicted by an ideal regressor? Justify.
R11: An ideal regressor would have plotted all the points on top of the line of slope 1 because all the points would have the predicted value equal to the true value, meaning that x (true value) equals y(predicted value) giving a line of slope 1.


Q12: Explain your validation procedure and comment on the true error
of your chosen model for unseen data.
R12: We used cross validation with 5 folds to validate the models. This is done by using Kfold from the sklearn libray (StratifiedKFold in the case of the Naive Bayes Classifier to have a balanced distribution of the samples) to split the training data into 5 different folds of training and validation data.
With those indexes we then train the models and validate 5 times, giving a training and validation error that is added to a variable.
In the end we calculate the mean of this errors by dividing them by the number of folds.This validation error gives us the possibility of choosing the best hypothesis to estimate the true error.
This true error is estimated with the test data that was not used for anything else and only for calculating the test error.
This test error (estimation of true error) is the best example of how the model would work with any data outside the training and validation procedures.



